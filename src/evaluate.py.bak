"""
Evaluate the trained drug recommender model using various metrics.
"""
import os
import numpy as np
import pandas as pd
import torch
from pathlib import Path
import json
import logging
from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple

from model import InteractionModel

def load_data(data_dir: str) -> Tuple[np.ndarray, np.ndarray, Dict, Dict]:
    """Load processed ChEMBL data and fingerprints."""
    fps = np.load(os.path.join(data_dir, 'fingerprints.npy'))
    activity_df = pd.read_csv(os.path.join(data_dir, 'bioactivity.csv'))
    
    # Create interaction matrix
    n_drugs = len(fps)
    n_proteins = activity_df['target_chembl_id'].nunique()
    interactions = np.zeros((n_drugs, n_proteins), dtype=np.float32)
    
    # Map IDs to indices
    drug_to_idx = {id_: idx for idx, id_ in enumerate(activity_df['molecule_chembl_id'].unique())}
    protein_to_idx = {id_: idx for idx, id_ in enumerate(activity_df['target_chembl_id'].unique())}
    
    # Fill interaction matrix
    for _, row in activity_df.iterrows():
        drug_idx = drug_to_idx[row['molecule_chembl_id']]
        protein_idx = protein_to_idx[row['target_chembl_id']]
        interactions[drug_idx, protein_idx] = 1.0
    
    return fps, interactions, drug_to_idx, protein_to_idx

def find_optimal_threshold(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """Find optimal classification threshold using F1 score."""
    thresholds = np.linspace(0.01, 0.99, 50)
    best_f1 = 0
    best_threshold = 0.5
    
    for threshold in thresholds:
        y_pred_binary = (y_pred >= threshold).astype(float)
        f1 = 2 * (precision_score(y_true.flatten(), y_pred_binary.flatten(), zero_division=0) * 
                  recall_score(y_true.flatten(), y_pred_binary.flatten(), zero_division=0)) / \
             (precision_score(y_true.flatten(), y_pred_binary.flatten(), zero_division=0) + 
              recall_score(y_true.flatten(), y_pred_binary.flatten(), zero_division=0) + 1e-10)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    return best_threshold

def compute_ranking_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """Compute ranking-based metrics (MRR, MAP)."""
    mrr_scores = []
    ap_scores = []
    
    for i in range(y_true.shape[0]):
        true_targets = np.where(y_true[i] > 0)[0]
        if len(true_targets) > 0:
            # Sort predictions in descending order
            sorted_indices = np.argsort(y_pred[i])[::-1]
            
            # MRR - reciprocal rank of first relevant item
            first_rel_rank = np.min([np.where(sorted_indices == t)[0][0] for t in true_targets]) + 1
            mrr_scores.append(1.0 / first_rel_rank)
            
            # MAP - mean precision at each relevant item
            precisions = []
            n_correct = 0
            for j, idx in enumerate(sorted_indices, 1):
                if idx in true_targets:
                    n_correct += 1
                    precisions.append(n_correct / j)
            ap_scores.append(np.mean(precisions))
    
    return {
        'mrr': np.mean(mrr_scores) if mrr_scores else float('nan'),
        'map': np.mean(ap_scores) if ap_scores else float('nan')
    }

def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, threshold: float = None) -> Dict[str, float]:
    """Compute various evaluation metrics."""
    if threshold is None:
        threshold = find_optimal_threshold(y_true, y_pred)
    
    # Binary predictions
    y_pred_binary = (y_pred >= threshold).astype(float)
    
    # Basic metrics
    precision = precision_score(y_true.flatten(), y_pred_binary.flatten(), zero_division=0)
    recall = recall_score(y_true.flatten(), y_pred_binary.flatten(), zero_division=0)
    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)
    
    # ROC AUC
    try:
        auroc = roc_auc_score(y_true.flatten(), y_pred.flatten())
    except:
        auroc = float('nan')
    
    # Average Precision (area under precision-recall curve)
    try:
        aupr = average_precision_score(y_true.flatten(), y_pred.flatten())
    except:
        aupr = float('nan')
    
    # NDCG@k for each protein
    ndcg_scores = []
    for i in range(y_true.shape[1]):
        true_relevance = y_true[:, i]
        predicted_scores = y_pred[:, i]
        if true_relevance.sum() > 0:  # Only compute NDCG if there are positive examples
            ranking = np.argsort(predicted_scores)[::-1]
            ndcg = ndcg_at_k(true_relevance[ranking], k=10)
            ndcg_scores.append(ndcg)
    
    ndcg_10 = np.mean(ndcg_scores) if ndcg_scores else float('nan')
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auroc': auroc,
        'aupr': aupr,
        'ndcg@10': ndcg_10
    }

def compute_ranking_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """Compute ranking-based metrics (MRR, MAP)."""
    mrr_scores = []
    ap_scores = []
    
    for i in range(y_true.shape[0]):
        true_targets = np.where(y_true[i] > 0)[0]
        if len(true_targets) > 0:
            # Sort predictions in descending order
            sorted_indices = np.argsort(y_pred[i])[::-1]
            
            # MRR - reciprocal rank of first relevant item
            first_rel_rank = np.min([np.where(sorted_indices == t)[0][0] for t in true_targets]) + 1
            mrr_scores.append(1.0 / first_rel_rank)
            
            # MAP - mean precision at each relevant item
            precisions = []
            n_correct = 0
            for j, idx in enumerate(sorted_indices, 1):
                if idx in true_targets:
                    n_correct += 1
                    precisions.append(n_correct / j)
            ap_scores.append(np.mean(precisions))
    
    return {
        'mrr': np.mean(mrr_scores) if mrr_scores else float('nan'),
        'map': np.mean(ap_scores) if ap_scores else float('nan')
    }

def plot_prediction_distribution(y_true: np.ndarray, y_pred: np.ndarray, threshold: float, save_path: str):
    """Plot distribution of predictions for positive and negative examples."""
    plt.figure(figsize=(10, 6))
    plt.hist(y_pred[y_true == 0].flatten(), bins=50, alpha=0.5, label='Negative Examples', density=True)
    plt.hist(y_pred[y_true == 1].flatten(), bins=50, alpha=0.5, label='Positive Examples', density=True)
    plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold = {threshold:.3f}')
    plt.xlabel('Prediction Score')
    plt.ylabel('Density')
    plt.title('Distribution of Prediction Scores')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()

def plot_precision_recall_curve(y_true: np.ndarray, y_pred: np.ndarray, save_path: str):
    """Plot precision-recall curve."""
    from sklearn.metrics import precision_recall_curve
    precision, recall, thresholds = precision_recall_curve(y_true.flatten(), y_pred.flatten())
    
    plt.figure(figsize=(10, 6))
    plt.plot(recall, precision)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()

def ndcg_at_k(rel: np.ndarray, k: int) -> float:
    """Compute NDCG@k for a single list of relevance scores."""
    dcg = 0.0
    idcg = 0.0
    rel = np.asarray(rel)[:k]
    
    for i, r in enumerate(rel, 1):
        dcg += r / np.log2(i + 1)
    
    ideal_rel = np.sort(rel)[::-1]
    for i, r in enumerate(ideal_rel, 1):
        idcg += r / np.log2(i + 1)
    
    return dcg / (idcg + 1e-10)

def plot_prediction_distribution(y_true: np.ndarray, y_pred: np.ndarray, threshold: float, save_path: str):
    """Plot distribution of predictions for positive and negative examples."""
    plt.figure(figsize=(10, 6))
    plt.hist(y_pred[y_true == 0].flatten(), bins=50, alpha=0.5, label='Negative Examples', density=True)
    plt.hist(y_pred[y_true == 1].flatten(), bins=50, alpha=0.5, label='Positive Examples', density=True)
    plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold = {threshold:.3f}')
    plt.xlabel('Prediction Score')
    plt.ylabel('Density')
    plt.title('Distribution of Prediction Scores')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()

def plot_precision_recall_curve(y_true: np.ndarray, y_pred: np.ndarray, save_path: str):
    """Plot precision-recall curve."""
    from sklearn.metrics import precision_recall_curve
    precision, recall, thresholds = precision_recall_curve(y_true.flatten(), y_pred.flatten())
    
    plt.figure(figsize=(10, 6))
    plt.plot(recall, precision)
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.grid(True)
    plt.savefig(save_path)
    plt.close()

def main():
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    # Load data
    data_dir = 'data/chembl'
    fps, interactions, drug_to_idx, protein_to_idx = load_data(data_dir)
    logger.info(f"Loaded {len(fps)} drugs with {interactions.shape[1]} targets")
    
    # Setup train/test split with stratification
    from sklearn.model_selection import train_test_split
    
    # Create stratification labels based on number of interactions per drug
    n_interactions_per_drug = interactions.sum(axis=1)
    strat_labels = pd.qcut(n_interactions_per_drug, q=3, labels=False, duplicates='drop')
    
    # Split data ensuring both splits have similar distribution of interaction counts
    train_idx, test_idx = train_test_split(
        np.arange(len(fps)), 
        test_size=0.2, 
        random_state=42,
        stratify=strat_labels
    )
    
    # Verify splits
    n_pos_train = interactions[train_idx].sum()
    n_pos_test = interactions[test_idx].sum()
    logger.info(f"Train samples: {len(train_idx)}, Test samples: {len(test_idx)}")
    logger.info(f"Train positives: {n_pos_train}, Test positives: {n_pos_test}")
        
        X_train = fps[train_idx]
        X_test = fps[test_idx]
        y_train = interactions[train_idx]
        y_test = interactions[test_idx]
    
    # Load model checkpoint
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    checkpoint = torch.load('models/drug_recommender.pt', map_location=device)
    
    # Create model with same architecture
    config = checkpoint['config']
    model = InteractionModel(
        patient_dim=interactions.shape[1],
        drug_dim=fps.shape[1],
        latent_dim=config['hyperparameters']['latent_dim']
    ).to(device)
    
    # Load trained weights
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # Get predictions
    with torch.no_grad():
        X_test_tensor = torch.FloatTensor(X_test).to(device)
        dummy_protein = torch.zeros(len(X_test), interactions.shape[1]).to(device)
        pred, _, _ = model(dummy_protein, X_test_tensor)
        y_pred = torch.sigmoid(pred).cpu().numpy()
    
        # Find optimal threshold and compute metrics
        threshold = find_optimal_threshold(y_test, y_pred)
        metrics = compute_metrics(y_test, y_pred, threshold)
        
        # Add ranking metrics
        ranking_metrics = compute_ranking_metrics(y_test, y_pred)
        metrics.update(ranking_metrics)
        
        # Add threshold to metrics
        metrics['threshold'] = threshold
        
        all_metrics.append(metrics)
        all_thresholds.append(threshold)
        
        # Print fold results
        logger.info(f"\nFold {fold + 1} Metrics (threshold = {threshold:.4f}):")
        for metric, value in metrics.items():
            logger.info(f"{metric}: {value:.4f}")
            
        # Generate visualizations for this fold
        os.makedirs('models/plots', exist_ok=True)
        plot_prediction_distribution(
            y_test, y_pred, threshold,
            f'models/plots/prediction_dist_fold{fold+1}.png'
        )
        plot_precision_recall_curve(
            y_test, y_pred,
            f'models/plots/pr_curve_fold{fold+1}.png'
        )
    
    # Compute average metrics across folds
    avg_metrics = {}
    std_metrics = {}
    for metric in all_metrics[0].keys():
        values = [m[metric] for m in all_metrics]
        avg_metrics[metric] = float(np.mean(values))
        std_metrics[metric] = float(np.std(values))
    
    # Save results
    results = {
        'metrics': {
            'average': avg_metrics,
            'std': std_metrics,
            'per_fold': all_metrics
        },
        'config': config,
        'dataset_info': {
            'n_drugs': int(len(fps)),
            'n_targets': int(interactions.shape[1]),
            'n_interactions': int(float(interactions.sum())),
            'sparsity': float(1 - (interactions.sum() / (len(fps) * interactions.shape[1])))
        }
    }
    
    results_path = os.path.join('models', 'evaluation_results.json')
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    logger.info(f"\nSaved detailed results to {results_path}")
    
    # Additional analysis: Top predicted targets for each drug
    idx_to_protein = {v: k for k, v in protein_to_idx.items()}
    
    logger.info("\nExample Predictions:")
    for i in range(min(5, len(X_test))):
        true_targets = np.where(y_test[i] > 0)[0]
        pred_scores = y_pred[i]
        top_k = 5
        
        # Get top predicted targets
        top_pred_idx = np.argsort(pred_scores)[::-1][:top_k]
        
        logger.info(f"\nDrug {i+1}:")
        logger.info("True targets:")
        for idx in true_targets:
            logger.info(f"- {idx_to_protein[idx]}")
        
        logger.info("\nTop predicted targets:")
        for idx in top_pred_idx:
            logger.info(f"- {idx_to_protein[idx]}: {pred_scores[idx]:.4f}")

if __name__ == '__main__':
    main()